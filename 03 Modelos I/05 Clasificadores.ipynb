{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:30px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong><i>Conjunto de clasificadores robustos</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i> Bag of Words Ingles</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejemplo extraido de https://scikit-learn.org/ adaptado al curso*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CountVectorizer:*** implementa la tokenización como el recuento de ocurrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Entrenamiento***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Palabras detectadas*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    " print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oraciones procesadas**\n",
    "* This is the first document.\n",
    "* This document is the second document.\n",
    "* And this is the third one.\n",
    "* Is this the first document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cada término encontrado se le asigna un **índice** entero único correspondiente a una columna en la matriz resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Document one new.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i> Bag of Words Español</i></strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Este es el primer documento.',\n",
    "          'Este documento es el segundo documento.',\n",
    "          'Y este es el tercero.',\n",
    "          '¿Es este el primer documento?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento', 'el', 'es', 'este', 'primer', 'segundo', 'tercero']\n"
     ]
    }
   ],
   "source": [
    " print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 0 0]\n",
      " [2 1 1 1 0 1 0]\n",
      " [0 1 1 1 0 0 1]\n",
      " [1 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('este')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Algo completamente nuevo.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Primer documento nuevo.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Desventaja:*** modelo pierde informacion de contexto, oraciones en afirmativo y en interrogativo podrian ser consideradas como similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Bigrams:*** hasta cierto punto esto puede ser solucionado mediante el uso de este recurso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = vectorizer2.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oraciones procesadas**\n",
    "* This is the first document.\n",
    "* This document is the second document.\n",
    "* And this is the third one.\n",
    "* Is this the first document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Texto extenso a modelar***"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To Sherlock Holmes she is always the woman. \n",
    "I have seldom heard him mention her under any other name. \n",
    "In his eyes she eclipses and predominates the whole of her sex. \n",
    "It was not that he felt any emotion akin to love for Irene Adler. \n",
    "All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. \n",
    "He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. \n",
    "He never spoke of the softer passions, save with a gibeand a sneer. \n",
    "They were admirable things for the observer excellent for drawing the veil from men motives and actions. \n",
    "But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his. \n",
    "And yet there was but one woman to him, and that woman was the late Irene.\n",
    "Adler, of dubious and questionable memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dividing_into_sentences import read_text_file, preprocess_text, divide_into_sentences_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(filename):\n",
    "    sherlock_holmes_text = read_text_file(filename)\n",
    "    sherlock_holmes_text = preprocess_text(sherlock_holmes_text)\n",
    "    sentences = divide_into_sentences_nltk(sherlock_holmes_text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_sentence_vector(sentence, vectorizer):\n",
    "    new_sentence_vector = vectorizer.transform([sentence])\n",
    "    return new_sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(sentences):\n",
    "    vectorizer = CountVectorizer(max_df=0.6) # Se descarta lo que este por debajo de este valor\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return (vectorizer, X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram_vectorizer(sentences):\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    X = bigram_vectorizer.fit_transform(sentences)\n",
    "    return (bigram_vectorizer, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences(\"./data/sherlock_holmes_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vectorizer, X) = create_vectorizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 113)\t1\n",
      "  (0, 98)\t1\n",
      "  (0, 46)\t1\n",
      "  (0, 97)\t1\n",
      "  (0, 53)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 123)\t1\n",
      "  (1, 38)\t1\n",
      "  (1, 94)\t1\n",
      "  (1, 40)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 63)\t1\n",
      "  (1, 41)\t1\n",
      "  (1, 115)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 78)\t1\n",
      "  (1, 69)\t1\n",
      "  (2, 97)\t1\n",
      "  (2, 41)\t1\n",
      "  (2, 47)\t1\n",
      "  (2, 45)\t1\n",
      "  (2, 28)\t1\n",
      "  (2, 24)\t1\n",
      "  (2, 87)\t1\n",
      "  :\t:\n",
      "  (9, 85)\t1\n",
      "  (9, 56)\t1\n",
      "  (9, 14)\t1\n",
      "  (9, 66)\t1\n",
      "  (9, 20)\t1\n",
      "  (9, 106)\t1\n",
      "  (9, 102)\t1\n",
      "  (9, 70)\t1\n",
      "  (10, 113)\t1\n",
      "  (10, 123)\t2\n",
      "  (10, 43)\t1\n",
      "  (10, 108)\t1\n",
      "  (10, 75)\t1\n",
      "  (10, 118)\t2\n",
      "  (10, 107)\t1\n",
      "  (10, 52)\t1\n",
      "  (10, 4)\t1\n",
      "  (10, 76)\t1\n",
      "  (10, 15)\t1\n",
      "  (10, 126)\t1\n",
      "  (10, 109)\t1\n",
      "  (10, 55)\t1\n",
      "  (10, 23)\t1\n",
      "  (10, 88)\t1\n",
      "  (10, 60)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(X.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_the_', 'abhorrent', 'actions', 'adjusted', 'adler', 'admirable', 'admirably', 'admit', 'akin', 'all', 'always', 'any', 'as', 'balanced', 'be', 'but', 'cold', 'crack', 'delicate', 'distracting', 'disturbing', 'doubt', 'drawing', 'dubious', 'eclipses', 'emotion', 'emotions', 'excellent', 'eyes', 'factor', 'false', 'felt', 'finely', 'for', 'from', 'gibe', 'grit', 'has', 'have', 'he', 'heard', 'her', 'high', 'him', 'himself', 'his', 'holmes', 'in', 'instrument', 'into', 'introduce', 'intrusions', 'irene', 'is', 'it', 'late', 'lenses', 'love', 'lover', 'machine', 'memory', 'men', 'mental', 'mention', 'might', 'mind', 'more', 'most', 'motives', 'name', 'nature', 'never', 'not', 'observer', 'observing', 'of', 'one', 'or', 'other', 'own', 'particularly', 'passions', 'perfect', 'placed', 'position', 'power', 'precise', 'predominates', 'questionable', 'reasoner', 'reasoning', 'results', 'save', 'seen', 'seldom', 'sensitive', 'sex', 'she', 'sherlock', 'sneer', 'softer', 'spoke', 'strong', 'such', 'take', 'temperament', 'than', 'that', 'the', 'there', 'they', 'things', 'throw', 'to', 'trained', 'under', 'upon', 'veil', 'was', 'were', 'which', 'whole', 'with', 'woman', 'world', 'would', 'yet']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\"\n",
    "new_sentence_vector = get_new_sentence_vector(new_sentence, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', 'and', 'that', 'woman', 'was', 'the', 'late', 'irene', 'adler', 'of', 'dubious', 'and', 'questionable', 'memory']\n"
     ]
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "print(analyze(new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 60)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 88)\t1\n",
      "  (0, 107)\t1\n",
      "  (0, 108)\t1\n",
      "  (0, 109)\t1\n",
      "  (0, 113)\t1\n",
      "  (0, 118)\t2\n",
      "  (0, 123)\t2\n",
      "  (0, 126)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 1 0 0 0 1 0 0 0 0 2 0 0 0 0 2 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_the_', 'abhorrent', 'actions', 'adjusted', 'adler', 'admirable', 'admirably', 'admit', 'akin', 'all', 'always', 'any', 'as', 'balanced', 'be', 'but', 'cold', 'crack', 'delicate', 'distracting', 'disturbing', 'doubt', 'drawing', 'dubious', 'eclipses', 'emotion', 'emotions', 'excellent', 'eyes', 'factor', 'false', 'felt', 'finely', 'for', 'from', 'gibe', 'grit', 'has', 'have', 'he', 'heard', 'her', 'high', 'him', 'himself', 'his', 'holmes', 'in', 'instrument', 'into', 'introduce', 'intrusions', 'irene', 'is', 'it', 'late', 'lenses', 'love', 'lover', 'machine', 'memory', 'men', 'mental', 'mention', 'might', 'mind', 'more', 'most', 'motives', 'name', 'nature', 'never', 'not', 'observer', 'observing', 'of', 'one', 'or', 'other', 'own', 'particularly', 'passions', 'perfect', 'placed', 'position', 'power', 'precise', 'predominates', 'questionable', 'reasoner', 'reasoning', 'results', 'save', 'seen', 'seldom', 'sensitive', 'sex', 'she', 'sherlock', 'sneer', 'softer', 'spoke', 'strong', 'such', 'take', 'temperament', 'than', 'that', 'the', 'there', 'they', 'things', 'throw', 'to', 'trained', 'under', 'upon', 'veil', 'was', 'were', 'which', 'whole', 'with', 'woman', 'world', 'would', 'yet']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bigram_vectorizer, X) = create_bigram_vectorizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 269)\t1\n",
      "  (0, 229)\t1\n",
      "  (0, 118)\t1\n",
      "  (0, 226)\t1\n",
      "  (0, 136)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 299)\t1\n",
      "  (0, 275)\t1\n",
      "  (0, 230)\t1\n",
      "  (0, 119)\t1\n",
      "  (0, 228)\t1\n",
      "  (0, 137)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 93)\t1\n",
      "  (1, 221)\t1\n",
      "  (1, 101)\t1\n",
      "  (1, 108)\t1\n",
      "  (1, 156)\t1\n",
      "  (1, 103)\t1\n",
      "  (1, 278)\t1\n",
      "  (1, 31)\t1\n",
      "  (1, 190)\t1\n",
      "  (1, 167)\t1\n",
      "  :\t:\n",
      "  (10, 307)\t1\n",
      "  (10, 261)\t1\n",
      "  (10, 141)\t1\n",
      "  (10, 60)\t1\n",
      "  (10, 210)\t1\n",
      "  (10, 151)\t1\n",
      "  (10, 30)\t1\n",
      "  (10, 308)\t1\n",
      "  (10, 262)\t1\n",
      "  (10, 285)\t1\n",
      "  (10, 45)\t1\n",
      "  (10, 187)\t1\n",
      "  (10, 300)\t1\n",
      "  (10, 271)\t1\n",
      "  (10, 109)\t1\n",
      "  (10, 251)\t1\n",
      "  (10, 301)\t1\n",
      "  (10, 288)\t1\n",
      "  (10, 253)\t1\n",
      "  (10, 142)\t1\n",
      "  (10, 8)\t1\n",
      "  (10, 180)\t1\n",
      "  (10, 61)\t1\n",
      "  (10, 27)\t1\n",
      "  (10, 211)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(X.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_the_', '_the_ woman', 'abhorrent', 'abhorrent to', 'actions', 'adjusted', 'adjusted temperament', 'adler', 'adler of', 'admirable', 'admirable things', 'admirably', 'admirably balanced', 'admit', 'admit such', 'akin', 'akin to', 'all', 'all emotions', 'all his', 'always', 'always _the_', 'and', 'and actions', 'and finely', 'and observing', 'and predominates', 'and questionable', 'and sneer', 'and that', 'and yet', 'any', 'any emotion', 'any other', 'as', 'as his', 'as lover', 'balanced', 'balanced mind', 'be', 'be more', 'but', 'but admirably', 'but as', 'but for', 'but one', 'cold', 'cold precise', 'crack', 'crack in', 'delicate', 'delicate and', 'distracting', 'distracting factor', 'disturbing', 'disturbing than', 'doubt', 'doubt upon', 'drawing', 'drawing the', 'dubious', 'dubious and', 'eclipses', 'eclipses and', 'emotion', 'emotion akin', 'emotion in', 'emotions', 'emotions and', 'excellent', 'excellent for', 'eyes', 'eyes she', 'factor', 'factor which', 'false', 'false position', 'felt', 'felt any', 'finely', 'finely adjusted', 'for', 'for drawing', 'for irene', 'for the', 'from', 'from men', 'gibe', 'gibe and', 'grit', 'grit in', 'has', 'has seen', 'have', 'have placed', 'have seldom', 'he', 'he felt', 'he never', 'he was', 'he would', 'heard', 'heard him', 'her', 'her sex', 'her under', 'high', 'high power', 'him', 'him and', 'him mention', 'himself', 'himself in', 'his', 'his cold', 'his eyes', 'his mental', 'his own', 'holmes', 'holmes she', 'in', 'in false', 'in his', 'in nature', 'in one', 'in sensitive', 'instrument', 'instrument or', 'into', 'into his', 'introduce', 'introduce distracting', 'intrusions', 'intrusions into', 'irene', 'irene adler', 'is', 'is always', 'it', 'it the', 'it was', 'late', 'late irene', 'lenses', 'lenses would', 'love', 'love for', 'lover', 'lover he', 'machine', 'machine that', 'memory', 'men', 'men motives', 'mental', 'mental results', 'mention', 'mention her', 'might', 'might throw', 'mind', 'more', 'more disturbing', 'most', 'most perfect', 'motives', 'motives and', 'name', 'nature', 'nature such', 'never', 'never spoke', 'not', 'not be', 'not that', 'observer', 'observer excellent', 'observing', 'observing machine', 'of', 'of dubious', 'of her', 'of his', 'of the', 'one', 'one of', 'one particularly', 'one woman', 'or', 'or crack', 'other', 'other name', 'own', 'own delicate', 'own high', 'particularly', 'particularly were', 'passions', 'passions save', 'perfect', 'perfect reasoning', 'placed', 'placed himself', 'position', 'power', 'power lenses', 'precise', 'precise but', 'predominates', 'predominates the', 'questionable', 'questionable memory', 'reasoner', 'reasoner to', 'reasoning', 'reasoning and', 'results', 'save', 'save with', 'seen', 'seen but', 'seldom', 'seldom heard', 'sensitive', 'sensitive instrument', 'sex', 'she', 'she eclipses', 'she is', 'sherlock', 'sherlock holmes', 'sneer', 'softer', 'softer passions', 'spoke', 'spoke of', 'strong', 'strong emotion', 'such', 'such as', 'such intrusions', 'take', 'take it', 'temperament', 'temperament was', 'than', 'than strong', 'that', 'that he', 'that one', 'that the', 'that woman', 'the', 'the late', 'the most', 'the observer', 'the softer', 'the trained', 'the veil', 'the whole', 'the world', 'there', 'there was', 'they', 'they were', 'things', 'things for', 'throw', 'throw doubt', 'to', 'to admit', 'to him', 'to his', 'to introduce', 'to love', 'to sherlock', 'trained', 'trained reasoner', 'under', 'under any', 'upon', 'upon all', 'veil', 'veil from', 'was', 'was but', 'was not', 'was take', 'was the', 'was to', 'were', 'were abhorrent', 'were admirable', 'which', 'which might', 'whole', 'whole of', 'with', 'with gibe', 'woman', 'woman to', 'woman was', 'world', 'world has', 'would', 'would have', 'would not', 'yet', 'yet there']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"I had seen little of Holmes lately.\"\n",
    "new_sentence_vector = bigram_vectorizer.transform([new_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 118)\t1\n",
      "  (0, 179)\t1\n",
      "  (0, 219)\t1\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector)\n",
    "print(new_sentence_vector.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence1 = \" And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\"\n",
    "new_sentence_vector1 = vectorizer.transform([new_sentence1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 60)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 88)\t1\n",
      "  (0, 107)\t1\n",
      "  (0, 108)\t1\n",
      "  (0, 109)\t1\n",
      "  (0, 113)\t1\n",
      "  (0, 118)\t2\n",
      "  (0, 123)\t2\n",
      "  (0, 126)\t1\n",
      "[[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 1 0 0 0 1 0 0 0 0 2 0 0 0 0 2 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector1)\n",
    "print(new_sentence_vector1.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>CBOW</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Continuous Bag-of-Words Word2Vec:*** es una arquitectura para crear vectores de palabras que utiliza tanto palabras futuras como pasadas. La función objetivo de CBOW es:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](./img/clow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](./img/word2vec_diagrams.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Paper:*** [Efficient Estimation of Word Representations in Vector Space](https://paperswithcode.com/method/cbow-word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Codigo adaptado para la clase: ***fasttext*** quick start guide*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras.backend as K\n",
    "#import keras.backend.tensorflow_backend as K\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Lambda, Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgrams(sequence, vocabulary_size,\n",
    "              window_size=window_size, negative_samples=1., shuffle=True,\n",
    "              categorical=False, sampling_table=None, seed=None):\n",
    "    couples = []\n",
    "    labels = []\n",
    "    for i, wi in enumerate(sequence):\n",
    "        if not wi:\n",
    "            continue\n",
    "        if sampling_table is not None:\n",
    "            if sampling_table[wi] < random.random():\n",
    "                continue\n",
    "\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "        for j in range(window_start, window_end):\n",
    "            if j != i:\n",
    "                wj = sequence[j]\n",
    "                if not wj:\n",
    "                    continue\n",
    "                couples.append([wi, wj])\n",
    "                if categorical:\n",
    "                    labels.append([0, 1])\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "    if negative_samples > 0:\n",
    "        num_negative_samples = int(len(labels) * negative_samples)\n",
    "        words = [c[0] for c in couples]\n",
    "        random.shuffle(words)\n",
    "\n",
    "        couples += [[words[i % len(words)],\n",
    "                    random.randint(1, vocabulary_size - 1)]\n",
    "                    for i in range(num_negative_samples)]\n",
    "        if categorical:\n",
    "            labels += [[1, 0]] * num_negative_samples\n",
    "        else:\n",
    "            labels += [0] * num_negative_samples\n",
    "\n",
    "    if shuffle:\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 10e6)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(couples)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(labels)\n",
    "        \n",
    "    return couples, labels\n",
    "\n",
    "def generate_data_for_cbow(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    corpus = tokenizer.texts_to_sequences(corpus)\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "            x = pad_sequences(contexts, maxlen=maxlen)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/alice.txt'\n",
    "corpus = open(path, encoding=\"utf-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in corpus if sentence.count(' ') >= 2]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "V=len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "# inputs\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "w = Embedding(V, embedding_dim)(w_inputs)\n",
    "\n",
    "# context\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "c = Embedding(V, embedding_dim)(c_inputs)\n",
    "o = Dot(axes=2)([w, c])\n",
    "o = Reshape((1,), input_shape=(1, 1))(o)\n",
    "o = Activation('sigmoid')(o)\n",
    "\n",
    "sg_model = Model(inputs=[w_inputs, c_inputs], outputs=o)\n",
    "sg_model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=embedding_dim, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:20px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Entrenamiento</i></strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1109.1710671558976\n",
      "1 759.6094448566437\n",
      "2 702.9872647300363\n",
      "3 676.4245354682207\n",
      "4 652.0895389690995\n"
     ]
    }
   ],
   "source": [
    "for ite in range(5):\n",
    "    loss = 0.\n",
    "    for i, doc in enumerate(tokenizer.texts_to_sequences(corpus)):\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=5, negative_samples=5.)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        if x:\n",
    "            loss += sg_model.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 250932.1743850708\n",
      "1 250805.3332901001\n",
      "2 250678.60900306702\n",
      "3 250551.89191818237\n",
      "4 250425.04319000244\n"
     ]
    }
   ],
   "source": [
    "for ite in range(5):\n",
    "    loss  =  0.\n",
    "    for  x, y in generate_data_for_cbow(corpus, window_size, V):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:20px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Guardar los vectores generados</i></strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/sg_vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(V-1, embedding_dim))\n",
    "    vectors = sg_model.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cbow_vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(V-1, embedding_dim))\n",
    "    vectors = cbow.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:20px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Carga de vectores</i></strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sg_model = gensim.models.KeyedVectors.load_word2vec_format( open('./data/sg_vectors.txt', 'r'), binary=False)\n",
    "sg_model = gensim.models.KeyedVectors.load_word2vec_format('./data/sg_vectors.txt', binary=False, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cbow_model = gensim.models.KeyedVectors.load_word2vec_format(open('./data/cbow_vectors.txt', 'r'), binary=False)\n",
    "cbow_model = gensim.models.KeyedVectors.load_word2vec_format('./data/cbow_vectors.txt', binary=False, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hearts', 0.7326700091362),\n",
       " ('wildly', 0.6947518587112427),\n",
       " ('became', 0.6890791058540344),\n",
       " ('fire', 0.6887832880020142),\n",
       " ('cook', 0.6798040270805359),\n",
       " ('turning', 0.6791201233863831),\n",
       " ('queen\\x92s', 0.676758348941803),\n",
       " ('croquet', 0.6745284795761108),\n",
       " ('top', 0.6716916561126709),\n",
       " ('hush', 0.6687166094779968)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.most_similar(positive=['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('losing', 0.40262794494628906),\n",
       " ('needs', 0.3390108048915863),\n",
       " ('thoughts', 0.32639408111572266),\n",
       " ('whiles', 0.3247351348400116),\n",
       " ('search', 0.28745707869529724),\n",
       " ('positively', 0.2724922299385071),\n",
       " ('ones', 0.2723672091960907),\n",
       " ('sentence', 0.26836562156677246),\n",
       " ('stored', 0.26779720187187195),\n",
       " ('pack', 0.2639751434326172)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.most_similar(positive=['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thought', 0.7039021253585815),\n",
       " ('nonsense', 0.5913527607917786),\n",
       " ('\\x93it', 0.5814018845558167),\n",
       " ('doubtfully', 0.5789979100227356),\n",
       " ('cautiously', 0.574916422367096),\n",
       " ('rather', 0.5677836537361145),\n",
       " ('\\x93i\\x92m', 0.5670735239982605),\n",
       " ('perhaps', 0.5468480587005615),\n",
       " ('said', 0.5464442372322083),\n",
       " ('glad', 0.5305448174476624)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.most_similar(positive=['alice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('display', 0.388045072555542),\n",
       " ('a\\x97i\\x92m', 0.30560874938964844),\n",
       " ('guests', 0.2873057425022125),\n",
       " ('draw', 0.282014936208725),\n",
       " ('burning', 0.28113794326782227),\n",
       " ('profit', 0.27617281675338745),\n",
       " ('time', 0.2714170217514038),\n",
       " ('clearly', 0.26782727241516113),\n",
       " ('exclamation', 0.26630762219429016),\n",
       " ('\\x93tut', 0.25979748368263245)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.most_similar(positive=['alice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6171810030937195),\n",
       " ('queen\\x92s', 0.6143390536308289),\n",
       " ('lobster', 0.5978270173072815),\n",
       " ('hearts', 0.593090832233429),\n",
       " ('country', 0.5838220715522766),\n",
       " ('croquet', 0.5799708366394043),\n",
       " ('archive', 0.5592383146286011),\n",
       " ('march', 0.558046281337738),\n",
       " ('literary', 0.5445029735565186),\n",
       " ('king', 0.5424962043762207)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.most_similar(positive=['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pronounced', 0.3117009699344635),\n",
       " ('tidy', 0.29297709465026855),\n",
       " ('compilation', 0.29158133268356323),\n",
       " ('prosecute', 0.2840246856212616),\n",
       " ('expense', 0.27993249893188477),\n",
       " ('trouble', 0.278850257396698),\n",
       " ('sea\\x97\\x94', 0.27178046107292175),\n",
       " ('watch', 0.2663557231426239),\n",
       " ('theirs', 0.2655295133590698),\n",
       " ('change', 0.2634385824203491)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.most_similar(positive=['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Term-Frequency Inverse Document-Frequency - TF IDF</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un corpus de texto extenso, algunas palabras estarán muy presentes (por ejemplo, \"the\", \"a\", \"is\" en inglés), por lo que ***contienen muy poca información significativa sobre el contenido real del documento***. Si se pasa directamente a un clasificador, esos términos muy frecuentes ensombrecerían las frecuencias de términos más raros pero más interesantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from removing_stopwords import read_in_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stopwords_file_path = \"./data/stopwords.csv\"\n",
    "sentences = get_sentences(\"./data/sherlock_holmes_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_vectorizer(sentences):\n",
    "    #Create TF-IDF object\n",
    "    tfidf_char_vectorizer = TfidfVectorizer(analyzer='char_wb', max_df=0.90, max_features=200000,\n",
    "                                        min_df=0.05, use_idf=True, ngram_range=(1,3))\n",
    "    tfidf_char_vectorizer = tfidf_char_vectorizer.fit(sentences)\n",
    "    tfidf_matrix = tfidf_char_vectorizer.transform(sentences)\n",
    "    print(tfidf_matrix)\n",
    "    dense_matrix = tfidf_matrix.todense()\n",
    "    print(dense_matrix)\n",
    "    print(tfidf_char_vectorizer.get_feature_names())\n",
    "    analyze = tfidf_char_vectorizer.build_analyzer()\n",
    "    print(analyze(\"To Sherlock Holmes she is always _the_ woman.\"))\n",
    "    return (tfidf_char_vectorizer, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(sentences):\n",
    "    #Create TF-IDF object\n",
    "    stopword_list = read_in_csv(stopwords_file_path)\n",
    "    stemmed_stopwords = [tokenize_and_stem(stopword)[0] for stopword in stopword_list]\n",
    "    stopword_list = stopword_list + stemmed_stopwords\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.90, max_features=200000,\n",
    "                                        min_df=0.05, stop_words=stopword_list,\n",
    "                                        use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "    tfidf_vectorizer = tfidf_vectorizer.fit(sentences)\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(sentences)\n",
    "    print(tfidf_matrix)\n",
    "    dense_matrix = tfidf_matrix.todense()\n",
    "    print(dense_matrix)\n",
    "    print(tfidf_vectorizer.get_feature_names())\n",
    "    return (tfidf_vectorizer, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 195)\t0.2892833606818738\n",
      "  (0, 167)\t0.33843668854613723\n",
      "  (0, 166)\t0.33843668854613723\n",
      "  (0, 165)\t0.33843668854613723\n",
      "  (0, 84)\t0.33843668854613723\n",
      "  (0, 83)\t0.33843668854613723\n",
      "  (0, 82)\t0.33843668854613723\n",
      "  (0, 1)\t0.33843668854613723\n",
      "  (0, 0)\t0.33843668854613723\n",
      "  (1, 160)\t0.408248290463863\n",
      "  (1, 159)\t0.408248290463863\n",
      "  (1, 158)\t0.408248290463863\n",
      "  (1, 115)\t0.408248290463863\n",
      "  (1, 78)\t0.408248290463863\n",
      "  (1, 77)\t0.408248290463863\n",
      "  (2, 194)\t0.28867513459481287\n",
      "  (2, 193)\t0.28867513459481287\n",
      "  (2, 164)\t0.28867513459481287\n",
      "  (2, 146)\t0.28867513459481287\n",
      "  (2, 145)\t0.28867513459481287\n",
      "  (2, 144)\t0.28867513459481287\n",
      "  (2, 60)\t0.28867513459481287\n",
      "  (2, 59)\t0.28867513459481287\n",
      "  (2, 58)\t0.28867513459481287\n",
      "  (2, 51)\t0.28867513459481287\n",
      "  :\t:\n",
      "  (9, 57)\t0.18475579480398302\n",
      "  (9, 52)\t0.13888402122461266\n",
      "  (9, 39)\t0.18475579480398302\n",
      "  (9, 38)\t0.18475579480398302\n",
      "  (9, 37)\t0.18475579480398302\n",
      "  (9, 30)\t0.18475579480398302\n",
      "  (9, 29)\t0.18475579480398302\n",
      "  (9, 28)\t0.18475579480398302\n",
      "  (10, 199)\t0.23495715509359508\n",
      "  (10, 198)\t0.23495715509359508\n",
      "  (10, 197)\t0.23495715509359508\n",
      "  (10, 196)\t0.23495715509359508\n",
      "  (10, 195)\t0.40166564525678816\n",
      "  (10, 148)\t0.23495715509359508\n",
      "  (10, 147)\t0.23495715509359508\n",
      "  (10, 109)\t0.23495715509359508\n",
      "  (10, 96)\t0.23495715509359508\n",
      "  (10, 95)\t0.20083282262839408\n",
      "  (10, 94)\t0.20083282262839408\n",
      "  (10, 48)\t0.23495715509359508\n",
      "  (10, 47)\t0.23495715509359508\n",
      "  (10, 46)\t0.23495715509359508\n",
      "  (10, 11)\t0.23495715509359508\n",
      "  (10, 10)\t0.23495715509359508\n",
      "  (10, 9)\t0.20083282262839408\n",
      "[[0.33843669 0.33843669 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "['_the_', '_the_ woman', 'abhorr', 'abhorr cold', 'abhorr cold precis', 'action', 'adjust', 'adjust tempera', 'adjust tempera introduc', 'adler', 'adler dubious', 'adler dubious question', 'admir', 'admir balanc', 'admir balanc mind', 'admir observer—excel', 'admir observer—excel draw', 'admit', 'admit intrus', 'admit intrus own', 'akin', 'akin love', 'akin love iren', 'balanc', 'balanc mind', 'cold', 'cold precis', 'cold precis admir', 'crack', 'crack own', 'crack own high-pow', 'delic', 'delic fine', 'delic fine adjust', 'distract', 'distract factor', 'distract factor throw', 'disturb', 'disturb strong', 'disturb strong emot', 'doubt', 'doubt mental', 'doubt mental result', 'draw', 'draw veil', 'draw veil men', 'dubious', 'dubious question', 'dubious question memori', 'eclips', 'eclips predomin', 'eclips predomin whole', 'emot', 'emot abhorr', 'emot abhorr cold', 'emot akin', 'emot akin love', 'emot natur', 'eye', 'eye eclips', 'eye eclips predomin', 'factor', 'factor throw', 'factor throw doubt', 'fals', 'fals posit', 'felt', 'felt emot', 'felt emot akin', 'fine', 'fine adjust', 'fine adjust tempera', 'gibe', 'gibe sneer', 'grit', 'grit sensit', 'grit sensit instrument', 'heard', 'heard mention', 'high-pow', 'high-pow lens', 'high-pow lens disturb', 'holm', 'holm _the_', 'holm _the_ woman', 'instrument', 'instrument crack', 'instrument crack own', 'introduc', 'introduc distract', 'introduc distract factor', 'intrus', 'intrus own', 'intrus own delic', 'iren', 'iren adler', 'iren adler dubious', 'lens', 'lens disturb', 'lens disturb strong', 'love', 'love iren', 'love iren adler', 'lover', 'lover place', 'lover place fals', 'machin', 'machin world', 'machin world lover', 'memori', 'men', 'men ’', 'men ’ motiv', 'mental', 'mental result', 'mention', 'mind', 'motiv', 'motiv action', 'natur', 'observ', 'observ machin', 'observ machin world', 'observer—excel', 'observer—excel draw', 'observer—excel draw veil', 'own', 'own delic', 'own delic fine', 'own high-pow', 'own high-pow lens', 'passion', 'passion save', 'passion save gibe', 'perfect', 'perfect reason', 'perfect reason observ', 'place', 'place fals', 'place fals posit', 'posit', 'precis', 'precis admir', 'precis admir balanc', 'predomin', 'predomin whole', 'predomin whole sex', 'question', 'question memori', 'reason', 'reason admit', 'reason admit intrus', 'reason observ', 'reason observ machin', 'result', 'save', 'save gibe', 'save gibe sneer', 'seldom', 'seldom heard', 'seldom heard mention', 'sensit', 'sensit instrument', 'sensit instrument crack', 'sex', 'sherlock', 'sherlock holm', 'sherlock holm _the_', 'sneer', 'softer', 'softer passion', 'softer passion save', 'spoke', 'spoke softer', 'spoke softer passion', 'strong', 'strong emot', 'strong emot natur', 'take', 'take perfect', 'take perfect reason', 'tempera', 'tempera introduc', 'tempera introduc distract', 'throw', 'throw doubt', 'throw doubt mental', 'train', 'train reason', 'train reason admit', 'veil', 'veil men', 'veil men ’', 'whole', 'whole sex', 'woman', 'woman iren', 'woman iren adler', 'woman woman', 'woman woman iren', 'world', 'world lover', 'world lover place', '’', '’ motiv', '’ motiv action']\n"
     ]
    }
   ],
   "source": [
    "(vectorizer, matrix) = create_vectorizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sherlock', 'holm', '_the_', 'woman', 'sherlock holm', 'holm _the_', '_the_ woman', 'sherlock holm _the_', 'holm _the_ woman']\n"
     ]
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "print(analyze(\"To Sherlock Holmes she is always _the_ woman.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = newsgroups_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = newsgroups_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorias de las 20 fuentes de datos:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "___________________________\n",
      "Ejemplo de un email:\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "___________________________\n",
      "Ejemplos de Target:\n",
      "7\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "print (\"Categorias de las 20 fuentes de datos:\")\n",
    "print (newsgroups_train.target_names)\n",
    "print (\"___________________________\")\n",
    "print (\"Ejemplo de un email:\")\n",
    "print (x_train[0])\n",
    "print (\"___________________________\")\n",
    "print (\"Ejemplos de Target:\")\n",
    "print (y_train[0])\n",
    "print (newsgroups_train.target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    try:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    except:\n",
    "        tokens = tokens\n",
    "        \n",
    "    tagged_corpus = pos_tag(tokens)    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    \n",
    "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
    "\n",
    "    return pre_proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_train:\n",
    "    x_train_preprocessed.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_preprocessed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_test:\n",
    "    x_test_preprocessed.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),  stop_words='english', \n",
    "                             max_features= 10000,strip_accents='unicode',  norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Deep Learning modules</i></strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adadelta,Adam,RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:20px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Hyper parameters</i></strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337) \n",
    "nb_classes = 20\n",
    "batch_size = 64\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1000,input_shape= (10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 1000)              10001000  \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                25050     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                1020      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 20)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,527,570\n",
      "Trainable params: 10,527,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:20px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Model Training</i></strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "177/177 [==============================] - 20s 101ms/step - loss: 1.9555\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 18s 101ms/step - loss: 0.5966\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 18s 99ms/step - loss: 0.3058\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 17s 99ms/step - loss: 0.1807\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 18s 100ms/step - loss: 0.1159\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 17s 98ms/step - loss: 0.0923\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 18s 101ms/step - loss: 0.0657\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 18s 103ms/step - loss: 0.0634\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 18s 102ms/step - loss: 0.0483\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 18s 102ms/step - loss: 0.0416\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 18s 102ms/step - loss: 0.0372\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 18s 102ms/step - loss: 0.0359\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 18s 102ms/step - loss: 0.0382\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 19s 106ms/step - loss: 0.0289\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 18s 104ms/step - loss: 0.0312\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 18s 103ms/step - loss: 0.0266\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 18s 103ms/step - loss: 0.0219\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 18s 102ms/step - loss: 0.0239\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 18s 102ms/step - loss: 0.0257\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 18s 102ms/step - loss: 0.0217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c383b7520>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:20px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Model Prediction</i></strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 2s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "y_train_predclass = model.predict(x_train_2,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_predclass = model.predict(x_test_2,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.999\n",
      "Test accuracy: 0.802\n"
     ]
    }
   ],
   "source": [
    "print (\"Train accuracy: {}\". format(round(accuracy_score(y_train,np.argmax(y_train_predclass,axis=1)),3)))\n",
    "print (\"Test accuracy: {}\". format(round(accuracy_score(y_test,np.argmax(y_test_predclass,axis=1)),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.71      0.76       319\n",
      "           1       0.67      0.71      0.69       389\n",
      "           2       0.73      0.65      0.69       394\n",
      "           3       0.66      0.72      0.68       392\n",
      "           4       0.79      0.72      0.76       385\n",
      "           5       0.80      0.77      0.78       395\n",
      "           6       0.77      0.82      0.79       390\n",
      "           7       0.81      0.87      0.84       396\n",
      "           8       0.96      0.87      0.91       398\n",
      "           9       0.93      0.90      0.91       397\n",
      "          10       0.96      0.96      0.96       399\n",
      "          11       0.91      0.89      0.90       396\n",
      "          12       0.63      0.74      0.68       393\n",
      "          13       0.87      0.85      0.86       396\n",
      "          14       0.90      0.90      0.90       394\n",
      "          15       0.82      0.88      0.85       398\n",
      "          16       0.78      0.83      0.81       364\n",
      "          17       0.96      0.82      0.89       376\n",
      "          18       0.71      0.65      0.68       310\n",
      "          19       0.57      0.70      0.63       251\n",
      "\n",
      "    accuracy                           0.80      7532\n",
      "   macro avg       0.80      0.80      0.80      7532\n",
      "weighted avg       0.81      0.80      0.80      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Test Classification Report\\n\")\n",
    "print (classification_report(y_test,np.argmax(y_test_predclass,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Word Embeddings</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mecanismo que como resultado del entrenamiento de una ***red neuronal***, predice una palabra a partir de todas las demás palabras de la oración. Los vectores resultantes son similares para palabras que ocurren en contextos similares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_model_path = \"./models/40/model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = KeyedVectors.load_word2vec_format(w2vec_model_path, binary=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(word_vectors):\n",
    "    matrix = np.array(word_vectors)\n",
    "    centroid = np.mean(matrix[:,:], axis=0)\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(sentence, model):\n",
    "    word_vectors = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            word_vector = model.get_vector(word.lower())\n",
    "            word_vectors.append(word_vector)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(w2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.309647 -0.127936 -0.136244 -0.252969  0.410695  0.206325  0.119236\n",
      " -0.244745 -0.436801  0.058889  0.237439  0.247656  0.072103  0.044183\n",
      " -0.424878  0.367344  0.153287  0.343856  0.232269 -0.181432 -0.050021\n",
      "  0.225756  0.71465  -0.564166 -0.168468 -0.153668  0.300445 -0.220122\n",
      " -0.021261  0.25779  -0.581744  0.320341 -0.236189  0.224906  0.029358\n",
      " -0.295143  0.483847 -0.05832   0.010784  0.050842 -0.034141  0.420114\n",
      "  0.126926 -0.405974 -0.421415  0.006092 -0.137557  0.038477  0.100005\n",
      "  0.151401  0.287163 -0.433263 -0.249083 -0.057834  0.367427 -0.181977\n",
      "  0.31608   0.063203 -0.486009 -0.127354 -0.283149  0.028113 -0.150146\n",
      " -0.38704   0.033237  0.146932  0.470853 -0.151154  0.064424  0.146739\n",
      " -0.164267 -0.094909  0.443384 -0.055244  0.117268 -0.221496 -0.185951\n",
      "  0.056249 -0.176986 -0.449508  0.345431 -0.096014 -0.19798   0.117698\n",
      " -0.162563 -0.181655 -0.18644  -0.158727  0.595464  0.161437 -0.382661\n",
      "  0.148537  0.173535  0.370556 -0.346765  0.055452  0.024405 -0.002895\n",
      "  0.081445  0.354575]\n"
     ]
    }
   ],
   "source": [
    "print(model['holmes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sherlock', 0.8416915535926819), ('parker', 0.8099909424781799), ('moriarty', 0.8039607405662537), ('sawyer', 0.8002702593803406), ('moore', 0.7932804822921753), ('wolfe', 0.7923581600189209), ('hale', 0.7910093069076538), ('doyle', 0.7906038165092468), ('holmes.the', 0.7895271182060242), ('watson', 0.788769006729126), ('yates', 0.7882786393165588), ('stevenson', 0.7879440188407898), ('spencer', 0.7877693176269531), ('goodwin', 0.7866846323013306), ('baxter', 0.7864187359809875)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(['holmes'], topn=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n"
     ]
    }
   ],
   "source": [
    "sentence = \"It was not that he felt any emotion akin to love for Irene Adler.\"\n",
    "word_vectors = get_word_vectors(sentence, model)\n",
    "sentence_vector = get_sentence_vector(word_vectors)\n",
    "words = ['banana', 'apple', 'computer', 'strawberry']\n",
    "print(model.doesnt_match(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glass\n"
     ]
    }
   ],
   "source": [
    "word = \"cup\"\n",
    "words = ['glass', 'computer', 'pencil', 'watch']\n",
    "print(model.most_similar_to_given(word, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Word2vec Model</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo ***word2vec*** utiliza un modelo de red neuronal para aprender asociaciones de palabras de un gran corpus de texto. Una vez entrenado, dicho modelo puede detectar palabras sinónimas o sugerir palabras adicionales para una oración parcial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](./img/word2vec_translation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuente: Python Deep Learning Projects, curaduria para el presente curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\", \" \", raw)\n",
    "    words = clean.split()\n",
    "    return list(map(lambda x: x.lower(), words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preproceso a realizar con el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](./img/learning-word-vectors-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Texto extraido de Principles of Geology by Sir Charles Lyell, Project Gutenberg***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'http://www.gutenberg.org/files/33224/33224-0.txt'\n",
    "corpus_raw = requests.get(filepath).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 425633 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Sentence where each word is tokenized\n",
    "sentences = (sentence_to_wordlist(raw) for raw in raw_sentences if raw)\n",
    "sentences = list(sentences)\n",
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(f'The book corpus contains {token_count} tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definicion del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones\n",
    "num_features = 300\n",
    "\n",
    "# umbral minimo para considerar una palabra.\n",
    "min_word_count = 3\n",
    "\n",
    "# Definicon de tareas en paralelo\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Ventana \n",
    "context_size = 7\n",
    "\n",
    "# Bajada a disco\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Semilla.\n",
    "seed = 1\n",
    "\n",
    "model2vec = w2v.Word2Vec(sg=1, seed=seed, workers=num_workers, min_count=min_word_count,\n",
    "                         window=context_size, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2vec.build_vocab(list(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Nota:*** Aumentar el número de dimensiones conduce a una mejor generalización, pero también agrega más complejidad computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Nota:*** Parametro ***context_size***. Establece el límite superior para la distancia entre la predicción de palabras actual y objetivo dentro de una oración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2vec.train(sentences, total_examples=model2vec.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2vec.save('./models/sample.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar a: \"earth\":\n",
      "('crust', 0.7069793939590454)\n",
      "('globe', 0.6538490056991577)\n",
      "('orbit', 0.6276617050170898)\n",
      "('inequalities', 0.6221923232078552)\n",
      "('planet', 0.6116704940795898)\n",
      "('surface', 0.5956591963768005)\n",
      "('moon', 0.5919835567474365)\n",
      "('sun', 0.5879665017127991)\n",
      "('remodelled', 0.5859501361846924)\n",
      "('unevenness', 0.5838420987129211)\n",
      "\n",
      "Similar a: \"human\":\n",
      "('race', 0.6516473889350891)\n",
      "('art', 0.6346074938774109)\n",
      "('man', 0.6225540041923523)\n",
      "('population', 0.6143702864646912)\n",
      "('affairs', 0.6132031679153442)\n",
      "('industry', 0.607807993888855)\n",
      "('men', 0.5850975513458252)\n",
      "('beings', 0.5833526253700256)\n",
      "('organized', 0.5775673389434814)\n",
      "('wickedness', 0.5763292908668518)\n",
      "\n",
      "Contribucion Positiva y Negativa:\n",
      "('employed', 0.8177217841148376)\n",
      "('sun', 0.8086581826210022)\n",
      "('crust', 0.7970605492591858)\n",
      "('remodelled', 0.7948926091194153)\n",
      "('attendant', 0.7932011485099792)\n",
      "('sound', 0.792156994342804)\n",
      "('globe', 0.7861462235450745)\n",
      "('god', 0.7815254926681519)\n",
      "('spirits', 0.7794240117073059)\n",
      "('rays', 0.7781361937522888)\n"
     ]
    }
   ],
   "source": [
    "print('Similar a: \"earth\":')\n",
    "for sWord in model2vec.wv.most_similar(\"earth\"):\n",
    "    print(sWord)\n",
    "    \n",
    "print('\\nSimilar a: \"human\":')\n",
    "for sWord in model2vec.wv.most_similar(\"human\"):\n",
    "    print(sWord)\n",
    "    \n",
    "print('\\nContribucion Positiva y Negativa:')\n",
    "for sWord in model2vec.wv.most_similar_cosmul(positive=['earth', 'moon'], negative=['orbit']):\n",
    "    print(sWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Train Word2Vec</i></strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from nltk.tokenize import  word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bag_of_words import get_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_path = \"models/word2vec.model\"\n",
    "books_dir = \"books/\"\n",
    "evaluation_file = \"test/questions-words.txt\"\n",
    "pretrained_model_path = \"models/40/model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(words, word2vec_model_path):\n",
    "    #model = gensim.models.Word2Vec(\n",
    "    #    words,\n",
    "    #    size=50,\n",
    "    #    window=7,\n",
    "    #    min_count=1,\n",
    "    #    workers=10)\n",
    "    model = gensim.models.Word2Vec(words, window=5, min_count=5)\n",
    "    model.train(words, total_examples=len(words), epochs=200)\n",
    "    pickle.dump(model, open(word2vec_model_path, 'wb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_book_sentences(directory):\n",
    "    text_files = [join(directory, f) for f in listdir(directory) if isfile(join(directory, f)) and \".rtf\" in f]\n",
    "    all_sentences = []\n",
    "    for text_file in text_files:\n",
    "        sentences = get_sentences(text_file)\n",
    "        all_sentences = all_sentences + sentences\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(w1):\n",
    "    model = pickle.load(open(word2vec_model_path, 'rb'))\n",
    "    #words = list(model.wv.vocab)\n",
    "    words = list(model.wv.index_to_key)\n",
    "    #print(words)\n",
    "    words = model.wv.most_similar(w1, topn=10)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, filename):\n",
    "    return model.wv.accuracy(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_all_book_sentences(books_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [word_tokenize(s.lower()) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_word2vec(sentences,word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('banks', 0.6390682458877563), ('mountains', 0.6331812739372253), ('woods', 0.6327189803123474), ('illinois', 0.6184098720550537), ('avenue', 0.6137031316757202), ('stream', 0.6122629642486572), ('raft', 0.5993295907974243), ('shore', 0.5992862582206726), ('hill', 0.5925582647323608), ('island', 0.5775993466377258)]\n"
     ]
    }
   ],
   "source": [
    "oneWord = \"river\"\n",
    "test_model(oneWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(word2vec_model_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Word2Vec Español</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Entrenamiento de wor2vect en españo, ejemplo adaptado para la clase](https://github.com/dccuchile/spanish-word-embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente s epuede acceder a [Word vectors for 157 languages](https://fasttext.cc/docs/en/crawl-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors_file_vec = './data/fasttext-sbwc.3.6.e20.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buscar analogias o palabras que tienen un contexto similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference: KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reina', 0.9141532778739929),\n",
       " ('infanta', 0.8582409620285034),\n",
       " ('berenguela', 0.8470728993415833),\n",
       " ('princesa', 0.8445042371749878),\n",
       " ('consorte', 0.835599422454834),\n",
       " ('emperatriz', 0.8247664570808411),\n",
       " ('regente', 0.8239888548851013),\n",
       " ('infantas', 0.8104740381240845),\n",
       " ('hermanastra', 0.8072930574417114),\n",
       " ('regencia', 0.8037239909172058)]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['rey','mujer'],negative=['hombre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actriz', 0.9687139391899109),\n",
       " ('compositora', 0.855713427066803),\n",
       " ('cantante', 0.8482002019882202),\n",
       " ('actrices', 0.845941424369812),\n",
       " ('dramaturga', 0.8354867696762085),\n",
       " ('presentadora', 0.8346402645111084),\n",
       " ('bailarina', 0.8301039934158325),\n",
       " ('coprotagonista', 0.8284398317337036),\n",
       " ('guionista', 0.828334629535675),\n",
       " ('cantautora', 0.8273791670799255)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['actor','mujer'],negative=['hombre'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hija', 0.9641352295875549),\n",
       " ('esposa', 0.911634087562561),\n",
       " ('madre', 0.9057635068893433),\n",
       " ('nieta', 0.8976945877075195),\n",
       " ('hermanastra', 0.8958925604820251)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['hijo','mujer'],negative=['hombre'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nuera', 0.8991931080818176),\n",
       " ('cuñada', 0.8967029452323914),\n",
       " ('esposa', 0.8791162967681885),\n",
       " ('hija', 0.8787108659744263),\n",
       " ('suegra', 0.8752366304397583),\n",
       " ('sobrina', 0.8678680658340454),\n",
       " ('hermanastra', 0.8615662455558777),\n",
       " ('viuda', 0.8587483167648315),\n",
       " ('yernos', 0.8577941656112671),\n",
       " ('nieta', 0.8574916124343872)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['yerno','mujer'],negative=['hombre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('juega', 0.927038848400116),\n",
       " ('jugará', 0.9030497670173645),\n",
       " ('juegue', 0.8957996368408203),\n",
       " ('jugando', 0.8832089304924011),\n",
       " ('juegan', 0.868077278137207),\n",
       " ('jugado', 0.8658615946769714),\n",
       " ('jugó', 0.8645128607749939),\n",
       " ('juegas', 0.8533657789230347),\n",
       " ('jugaría', 0.8508267402648926),\n",
       " ('jugara', 0.8470849394798279)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['jugar','canta'],negative=['cantar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jugaría', 1.002570629119873),\n",
       " ('jugarían', 0.9512909650802612),\n",
       " ('jugara', 0.9422452449798584),\n",
       " ('disputaría', 0.918655276298523),\n",
       " ('jugará', 0.908361554145813),\n",
       " ('jugaran', 0.8989545106887817),\n",
       " ('jugase', 0.8874877095222473),\n",
       " ('disputarían', 0.8822468519210815),\n",
       " ('jugó', 0.8740343451499939),\n",
       " ('ficharía', 0.8733251094818115)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['jugar','cantaría'],negative=['cantar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yendo', 0.907002329826355),\n",
       " ('ido', 0.8450857996940613),\n",
       " ('saliendo', 0.832144021987915),\n",
       " ('caminando', 0.8135581612586975),\n",
       " ('yéndose', 0.8133329153060913),\n",
       " ('acercando', 0.8035196661949158),\n",
       " ('iremos', 0.8023999333381653),\n",
       " ('marchando', 0.8001841902732849),\n",
       " ('parando', 0.7995682954788208),\n",
       " ('irá', 0.7987060546875)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['ir','jugando'],negative=['jugar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caracas', 0.9048638343811035),\n",
       " ('barinas', 0.871845543384552),\n",
       " ('brión', 0.8565776944160461),\n",
       " ('cojedes', 0.851475715637207),\n",
       " ('cumaná', 0.8507834076881409),\n",
       " ('guanare', 0.8507249355316162),\n",
       " ('maturín', 0.8474243879318237),\n",
       " ('mariño', 0.8468520641326904),\n",
       " ('barquisimeto', 0.8451403379440308),\n",
       " ('falcón', 0.8430415987968445)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['santiago','venezuela'],negative=['chile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cuba', 0.9638005495071411),\n",
       " ('venezuela', 0.8891815543174744),\n",
       " ('colombia', 0.876230001449585),\n",
       " ('cubana', 0.8471046686172485),\n",
       " ('nicaragua', 0.8443881273269653),\n",
       " ('cubanos', 0.8370179533958435),\n",
       " ('ecuador', 0.8361554145812988),\n",
       " ('brasil', 0.8355840444564819),\n",
       " ('cubano', 0.8315702080726624),\n",
       " ('panamá', 0.8302189111709595)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_cosmul(positive=['habana','chile'],negative=['santiago'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chile'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.most_similar_to_given('santiago', ['cuba','chile', 'brasil'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7982443"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.n_similarity('santiago', 'chile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9034706"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.n_similarity('chile', 'brasil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### palabra dentro de que está más lejana del resto de las palabras de la lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chile'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.doesnt_match(['blanco','azul','rojo','chile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'almuerzo'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.doesnt_match(['sol','luna','almuerzo','jupiter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:left;border-radius:5px;\">\n",
    "<strong><i>Bidirectional Encoder Representations from Transformers - Bert</i></strong></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***BERT*** utiliza Transformer, un mecanismo que aprende las relaciones contextuales entre palabras en un texto. En su forma básica, Transformer incluye dos mecanismos separados: un codificador que lee la entrada de texto y un decodificador que produce una predicción para la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dividing_into_sentences import read_text_file, divide_into_sentences_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_text_file(\"./data/sherlock_holmes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = divide_into_sentences_nltk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48ca7dd161c44e2b48969698b790f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)821d1/.gitattributes:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce82787047404112ba8a6329b47e21cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fd138e3f294055b7abb62de011562c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8d01e821d1/README.md:   0%|          | 0.00/3.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf1bf0e04934a128b03456800a0b709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d1/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee0e4b96ccd4aefb6ec62db65c00419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)01e821d1/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53df3830ca91450e96050d3b0266380f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb5b6e54d684595a87b2d68034afa72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3561448b4223481a85e942cbd1c137d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d392092fce0340c2aa7df22d3d8b3be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a65732eb6004dd98906ebd7360b65ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)821d1/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624b4d4f397648a9a6b23b4cb0f87457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb91689ebce426b8bb9c879297a3975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8d01e821d1/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef52091cea7414499adb1032f18951c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)1e821d1/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode([\"the beautiful lake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "[[-7.61982948e-02 -5.74669957e-01  1.08264244e+00  7.36554921e-01\n",
      "   5.51345766e-01 -9.39117789e-01 -2.80430317e-01 -5.41625559e-01\n",
      "   7.50949204e-01 -4.40971464e-01  5.31526744e-01 -5.41882992e-01\n",
      "   1.92792937e-01  3.44117552e-01  1.50266457e+00 -6.26990139e-01\n",
      "  -2.42829040e-01 -3.66734654e-01  5.57459831e-01 -2.21802637e-01\n",
      "  -9.69591737e-01 -4.38950121e-01 -7.93552458e-01 -5.84922850e-01\n",
      "  -1.55690759e-01  2.12004229e-01  4.02014196e-01 -2.63063848e-01\n",
      "   6.21910214e-01  5.97238004e-01  9.78123173e-02  7.20052481e-01\n",
      "  -4.66322720e-01  3.86450231e-01 -8.24903488e-01  1.09985709e+00\n",
      "  -3.59135151e-01 -4.31918919e-01  2.56567057e-02  5.73160291e-01\n",
      "   2.40237564e-01 -7.67570674e-01  9.38899815e-01 -3.60024393e-01\n",
      "  -8.77115607e-01 -2.47681111e-01 -8.65838528e-01  1.04203582e+00\n",
      "   3.65989417e-01 -6.47720471e-02 -7.04247296e-01  5.91089716e-03\n",
      "  -8.04807484e-01  2.21370250e-01 -1.79775149e-01  8.04759324e-01\n",
      "  -4.44356680e-01 -4.46378887e-01  7.55991489e-02 -2.17623949e-01\n",
      "   6.87522113e-01 -4.70606416e-01  7.68602073e-01  3.06246221e-01\n",
      "  -9.10274506e-01  6.28714979e-01  8.11625957e-01 -3.83971110e-02\n",
      "   2.41827324e-01 -3.13487589e-01  9.08201098e-01  9.62719694e-02\n",
      "  -4.04239953e-01  3.88271749e-01 -4.22080606e-01 -4.33438390e-01\n",
      "   7.79737294e-01 -1.52796358e-01  9.48364735e-01  9.40598130e-01\n",
      "   7.34893978e-01  6.64677501e-01  3.90085250e-01  1.34034204e+00\n",
      "   1.08232796e-01 -3.95383418e-01  2.27669522e-01  2.79989183e-01\n",
      "  -1.70993936e+00  4.19878960e-01  2.10107282e-01 -5.28873861e-01\n",
      "   7.26883173e-01 -1.09560513e+00 -1.37759866e-02  9.04117882e-01\n",
      "   3.81061360e-02 -2.46197656e-01  5.54346323e-01 -3.61581087e-01\n",
      "   6.56911612e-01 -1.13580966e+00 -3.63160491e-01  1.89225152e-01\n",
      "  -9.07701313e-01  4.76393163e-01  5.17776430e-01 -1.19440019e+00\n",
      "   9.81190026e-01 -4.90275472e-02 -1.50155067e-01  5.42353511e-01\n",
      "   7.50451982e-01  4.16537702e-01 -8.21163207e-02  1.74670771e-01\n",
      "  -1.52958238e+00  7.14822352e-01  2.90298164e-01  7.76944757e-01\n",
      "  -3.69304001e-01 -5.91734163e-02  4.75242548e-02 -5.92364743e-02\n",
      "   5.25073886e-01 -7.08114386e-01 -2.52816886e-01  1.29740512e+00\n",
      "  -1.28921819e+00  1.40943378e-01  2.23808557e-01  3.13144833e-01\n",
      "   5.11783957e-01 -2.58189049e-02 -2.22356860e-02  1.43362731e-01\n",
      "   3.71723741e-01 -8.38744938e-01 -1.68651581e+00  1.41473383e-01\n",
      "  -1.02585077e+00  2.03563377e-01  2.62083471e-01  7.15595335e-02\n",
      "   8.99083734e-01 -5.28086305e-01  4.57701594e-01 -1.53959632e-01\n",
      "  -1.18478224e-01 -8.72818828e-02  3.76766250e-02  7.07110584e-01\n",
      "  -7.49195755e-01  6.55253679e-02  5.15915871e-01  8.10095489e-01\n",
      "  -5.19098341e-01  2.18548805e-01  3.93635035e-01  6.09233081e-01\n",
      "   3.38943899e-01  9.46189523e-01 -1.22255230e+00 -1.30253047e-01\n",
      "   6.54754579e-01 -8.41753304e-01  8.06663036e-02  3.00383002e-01\n",
      "  -8.12576652e-01  5.76909781e-01  3.84841740e-01 -4.66208369e-01\n",
      "   2.26871781e-02 -1.82357639e-01  3.94846857e-01 -4.98396456e-01\n",
      "  -3.28549623e-01 -9.03554559e-01  8.20933282e-02 -1.04095912e+00\n",
      "  -2.30055735e-01 -3.92115921e-01  4.96069610e-01 -2.35530809e-01\n",
      "  -4.00794178e-01 -6.51569605e-01 -2.91195095e-01  1.30913809e-01\n",
      "   4.64919478e-01 -3.41870368e-01  2.17010409e-01 -1.27972937e+00\n",
      "  -4.89797182e-02 -2.56777465e-01  5.36798239e-01  1.87181994e-01\n",
      "  -1.41932175e-01  1.18326163e+00 -2.36010998e-01 -4.17852312e-01\n",
      "   7.82344222e-01  2.52221376e-01 -5.96156240e-01 -1.00841977e-01\n",
      "  -8.53248239e-01 -4.25653160e-02  1.98199764e-01 -4.74220335e-01\n",
      "  -7.92613149e-01 -5.80058508e-02 -5.30738831e-01 -4.08141650e-02\n",
      "   1.06413805e+00 -1.95783764e-01 -1.18166280e+00 -3.84621680e-01\n",
      "   1.83776543e-01  1.07722823e-02 -5.73451817e-01 -6.28654137e-02\n",
      "   3.68158817e-01  2.95115821e-02 -3.12129259e-01  7.59689689e-01\n",
      "   1.71070307e-01 -2.51229018e-01  1.21716118e+00 -1.22805558e-01\n",
      "  -1.25155675e+00  2.96520621e-01 -1.59362197e-01 -3.07772696e-01\n",
      "   7.38719702e-01 -3.21678430e-01  4.56360638e-01  6.06843352e-01\n",
      "  -1.03566930e-01  5.64707041e-01  1.14319158e+00  1.58940583e-01\n",
      "   4.00117069e-01 -1.40942529e-01  1.12744056e-01 -5.38726985e-01\n",
      "  -1.14471209e+00  2.97611415e-01  9.01936054e-01  1.17313758e-01\n",
      "   1.23890139e-01  1.81390733e-01  2.68770397e-01 -1.31405622e-01\n",
      "  -7.04876184e-02 -1.44888508e+00  6.95817530e-01  9.22024071e-01\n",
      "  -3.09454679e-01 -3.62523615e-01 -6.64731339e-02  3.39313924e-01\n",
      "  -1.24447785e-01 -6.02809191e-01 -1.31329417e+00 -2.52024382e-01\n",
      "  -1.04210687e+00 -1.28538680e+00 -4.44637448e-01  1.38656095e-01\n",
      "   2.24620149e-01  2.20691368e-01 -2.93923467e-01  3.47226597e-02\n",
      "  -2.68138856e-01  2.52771884e-01  8.13671589e-01 -2.96582282e-01\n",
      "   9.17571425e-01 -4.60455507e-01 -4.12234485e-01 -6.67553782e-01\n",
      "   8.68170932e-02  1.60345376e-01 -1.63572657e+00 -3.02731216e-01\n",
      "  -8.21481049e-01 -6.96784616e-01  1.73273057e-01  4.73819643e-01\n",
      "  -3.36352408e-01 -5.97198188e-01 -5.73053360e-01 -3.03225905e-01\n",
      "   4.93410587e-01 -9.86554503e-01  1.13781381e+00  2.27787286e-01\n",
      "   5.98226547e-01 -3.63631248e-01 -4.52825248e-01 -2.30590720e-02\n",
      "  -7.20068097e-01  5.94033122e-01  6.28289059e-02 -1.67542040e+00\n",
      "   6.84947371e-01  3.15438092e-01  1.30231798e+00  5.92271209e-01\n",
      "   5.25537610e-01 -5.70110500e-01 -2.30185896e-01  4.87735495e-02\n",
      "  -1.46310937e+00  2.91679353e-01  5.30582905e-01  4.55556631e-01\n",
      "  -1.05712295e+00  1.30680871e+00 -5.77883311e-02  9.02793556e-02\n",
      "  -4.90736574e-01  2.87302256e-01  2.68229276e-01  6.72244668e-01\n",
      "   1.36548257e+00  5.68947196e-03 -7.43366122e-01  1.22896087e+00\n",
      "  -6.58429503e-01  3.99420410e-01 -1.41790047e-01  2.50783771e-01\n",
      "   1.06889248e-01  1.99269176e-01 -1.92880541e-01  9.59797204e-01\n",
      "  -6.98571861e-01 -7.77557969e-01 -3.92655551e-01  3.84375304e-01\n",
      "   9.25756097e-01 -7.16386080e-01 -2.27031156e-01 -1.80262953e-01\n",
      "   8.75892043e-01  6.64648652e-01 -4.05386612e-02 -3.86093296e-02\n",
      "  -3.71947140e-01 -9.97883916e-01  7.03278065e-01 -4.95071888e-01\n",
      "  -1.86441198e-01 -5.57139099e-01 -5.84804833e-01  8.57470870e-01\n",
      "   9.73085105e-01 -4.38928232e-02  2.94231437e-02 -5.07004380e-01\n",
      "   5.19699693e-01  6.78202927e-01 -7.75590897e-01 -2.01014906e-01\n",
      "  -6.46363050e-02  1.69210017e-01 -3.15657496e-01 -6.38084859e-02\n",
      "   4.14385557e-01 -7.92832971e-01  7.81127810e-01  4.93745893e-01\n",
      "  -1.67177841e-01  3.25104177e-01  4.08136547e-01 -1.98128119e-01\n",
      "  -1.57883680e+00 -3.42697471e-01  2.81852007e-01  1.56250262e+00\n",
      "   5.42053469e-02 -3.72796386e-01  3.45635623e-01  2.87226029e-02\n",
      "   3.25957060e-01  4.45555538e-01 -2.36089583e-02 -3.59087080e-01\n",
      "   2.44493052e-01  1.08073816e-01 -2.71506910e-03  8.00017834e-01\n",
      "   1.64834082e-01 -7.84874409e-02  6.20462120e-01 -8.95981789e-01\n",
      "   9.13146019e-01  4.57475126e-01  4.43795621e-01  9.52291667e-01\n",
      "   5.36398590e-01  2.86281675e-01 -1.04407585e+00  6.62327409e-01\n",
      "  -4.04091030e-01 -4.31125730e-01  8.86067390e-01 -4.57727134e-01\n",
      "  -5.38968563e-01  4.08299387e-01  8.11756492e-01  3.18703443e-01\n",
      "  -3.00523937e-01 -2.06661105e-01 -7.86024392e-01 -5.08803844e-01\n",
      "   3.13619673e-01 -8.04826140e-01 -1.63368776e-01  8.13571751e-01\n",
      "   1.00145891e-01  4.00417715e-01  7.28013039e-01  5.52753627e-01\n",
      "   3.58506233e-01 -1.27685338e-01  1.08199084e+00 -3.31547618e-01\n",
      "  -2.09918231e-01  7.80606627e-01  4.58833128e-01  3.08563769e-01\n",
      "  -1.65190983e+00 -2.91312873e-01  2.00888962e-01 -7.44067878e-02\n",
      "  -8.22851881e-02 -7.58321166e-01  9.04613614e-01  1.21439576e-01\n",
      "   6.82858109e-01  2.89185971e-01  4.92762148e-01  3.60833347e-01\n",
      "   7.89760768e-01 -6.59668803e-01  2.36379936e-01 -5.94366908e-01\n",
      "  -3.39688249e-02  1.18596949e-01  1.19548421e-02  3.18183839e-01\n",
      "   2.10744977e-01 -9.42376554e-02  2.62368888e-01  6.69896305e-01\n",
      "  -1.56853154e-01  1.33296418e+00 -2.77229305e-02 -3.85722190e-01\n",
      "  -1.11986578e+00 -1.05736148e+00 -1.63957015e-01 -3.50023866e-01\n",
      "   5.55418789e-01  8.63675475e-02  2.35160999e-02  8.91652584e-01\n",
      "  -8.80947560e-02  4.73892577e-02  2.20449656e-01  4.89864573e-02\n",
      "  -3.63904893e-01  9.15377259e-01  6.34472538e-03  2.01183170e-01\n",
      "  -1.10757446e+00  1.27832305e+00  3.28630716e-01  1.38878071e+00\n",
      "   8.49062204e-02  1.00432679e-01  3.45135391e-01 -2.10845023e-01\n",
      "  -1.00216472e+00  5.91330826e-01  2.21506715e-01 -2.87747830e-01\n",
      "   1.82967290e-01 -7.25790083e-01 -4.80669960e-02 -2.03028485e-01\n",
      "   4.38333675e-02 -8.87951255e-01 -1.55841839e+00 -4.66842949e-01\n",
      "  -8.82860303e-01  7.36436725e-01  4.56631295e-02  7.06462562e-01\n",
      "  -4.05851841e-01  6.14355981e-01  5.78302264e-01 -1.18286157e+00\n",
      "  -2.77492497e-02 -7.50104010e-01 -3.99379909e-01 -7.83226669e-01\n",
      "  -2.69309342e-01  4.57274258e-01  4.13473040e-01  6.31410241e-01\n",
      "  -2.75248259e-01 -7.64646649e-01 -1.52929997e+00  3.75116229e-01\n",
      "  -6.66492105e-01  3.35962147e-01 -9.38857555e-01 -7.68575966e-01\n",
      "  -2.29994133e-01 -6.83000445e-01 -2.81757176e-01 -1.94841926e-03\n",
      "   3.74062717e-01  2.74860293e-01  6.72254503e-01  5.03118992e-01\n",
      "  -9.38203335e-01 -9.07175183e-01 -1.03282130e+00 -6.58715844e-01\n",
      "  -6.61198795e-01 -1.01838434e+00  3.76534551e-01  1.15521681e+00\n",
      "  -7.43075728e-01 -7.23348260e-01 -9.15834129e-01  5.64644158e-01\n",
      "   4.47726101e-01  6.31319404e-01  1.31402612e-02 -2.28386834e-01\n",
      "  -2.94719398e-01  6.34674430e-01  2.57517517e-01 -4.38146740e-01\n",
      "   6.84131503e-01  5.28099716e-01 -6.23339593e-01 -9.34386030e-02\n",
      "   6.24624133e-01 -1.16408491e+00 -9.08660412e-01  4.05242860e-01\n",
      "  -1.46841931e+00  3.72077703e-01 -7.48038888e-02  3.50935847e-01\n",
      "   8.60910058e-01  4.29133415e-01 -1.42858118e-01  1.14047788e-01\n",
      "   3.74011323e-02 -3.59079361e-01 -4.20894008e-03 -2.62010723e-01\n",
      "   3.37051690e-01  1.58273387e+00 -6.01672351e-01 -2.27530792e-01\n",
      "   1.19591546e+00  6.50878012e-01 -8.92955512e-02 -4.18905437e-01\n",
      "   1.57126713e+00 -3.31305444e-01 -2.72812933e-01  1.76590830e-01\n",
      "  -7.38163218e-02  3.12449396e-01 -8.97420920e-04 -2.13333219e-01\n",
      "  -1.53492761e+00  9.10113752e-02  2.28632957e-01 -1.51050723e+00\n",
      "  -3.86804998e-01  2.57271945e-01  8.85265470e-01 -4.24518198e-01\n",
      "  -3.86550933e-01  8.38054776e-01 -2.59798646e-01  5.40384173e-01\n",
      "  -2.86054820e-01  5.55787563e-01  6.21417522e-01 -1.23508886e-01\n",
      "   1.77527368e-02 -5.28726339e-01 -1.20459937e-01  2.90037930e-01\n",
      "   2.68551081e-01  1.10356309e-01 -1.02779269e+00 -9.56030011e-01\n",
      "   4.13787693e-01  2.25953817e-01 -1.31037736e+00 -2.35217929e+00\n",
      "  -1.86778277e-01 -1.20109832e+00 -4.42366421e-01 -8.44804645e-01\n",
      "  -3.90224278e-01  6.69506311e-01 -1.70657620e-01 -3.98013601e-03\n",
      "  -2.59338439e-01  3.82894099e-01 -5.44769645e-01 -6.13512993e-02\n",
      "  -2.61051655e-01 -3.40792805e-01 -9.61907879e-02  5.89640960e-02\n",
      "   3.95656884e-01 -5.89382887e-01  2.99506843e-01 -1.74587891e-01\n",
      "   2.27259636e-01 -8.18412006e-01 -7.32798100e-01  2.97234952e-01\n",
      "   2.28747159e-01  9.10663247e-01 -6.07201569e-02 -4.96814102e-01\n",
      "  -1.18733153e-01 -7.73223460e-01 -2.96834260e-01  6.71157241e-01\n",
      "  -3.38997662e-01 -9.53690335e-02 -5.03132224e-01  2.47434050e-01\n",
      "   4.27117407e-01  1.45333216e-01  8.05351436e-01 -8.57042432e-01\n",
      "   7.25859880e-01 -9.15387198e-02  2.53698379e-01  1.85381681e-01\n",
      "  -5.47812700e-01 -5.04314899e-01  1.36256099e+00 -8.62132549e-01\n",
      "   6.29888028e-02 -1.83469191e-01 -7.75031388e-01  3.41291338e-01\n",
      "   5.35521090e-01 -1.12125647e+00 -4.04197583e-03  1.34707898e-01\n",
      "   1.16122760e-01 -3.41756433e-01  9.87665355e-01 -3.03870738e-01\n",
      "  -1.51143417e-01 -1.04746366e+00 -4.28524435e-01 -4.62444216e-01\n",
      "   7.40947843e-01  9.54196155e-01 -8.21143508e-01  8.59959126e-01\n",
      "  -2.04597856e-03  3.10487092e-01  5.44805050e-01 -9.86975491e-01\n",
      "  -8.76125634e-01 -4.50644672e-01  4.49647993e-01  4.62937444e-01\n",
      "   3.74304950e-01  1.16935873e+00 -7.40902007e-01  2.80614555e-01\n",
      "  -2.25816760e-02 -1.63027799e+00  4.21418965e-01  8.17476273e-01\n",
      "   5.69595695e-01  8.26957226e-02  9.53551471e-01  3.85207832e-01\n",
      "  -1.18330574e+00  7.06426859e-01  6.79351926e-01 -1.24054754e+00\n",
      "  -1.43386686e+00 -5.74116349e-01 -2.98397303e-01 -4.57422346e-01\n",
      "  -6.46811366e-01 -1.45286724e-01  5.62383533e-01  2.07258508e-01\n",
      "   8.17670822e-01  6.23559773e-01  2.27319356e-02  1.39370754e-01\n",
      "  -6.04807496e-01  2.03888923e-01  4.75690931e-01 -4.73644555e-01\n",
      "  -6.28173172e-01  3.59570354e-01  2.00764805e-01  3.36479127e-01\n",
      "   2.37539411e-01 -3.53950143e-01  1.01170853e-01 -2.28819758e-01\n",
      "  -2.18078211e-01 -2.69777924e-01  9.67436910e-01  2.01076746e-01\n",
      "  -1.27904147e-01  7.99281836e-01 -1.35350242e-01 -3.20220202e-01\n",
      "   2.12029487e-01 -2.10678905e-01 -7.97776222e-01  3.23254287e-01\n",
      "  -9.47453141e-01  8.75162125e-01  4.30704206e-01 -2.36586288e-01\n",
      "  -7.85881877e-01  4.22886834e-02 -1.80885404e-01  2.76024337e-03\n",
      "  -6.27907574e-01  6.07978225e-01  1.15655208e+00  8.13454986e-01\n",
      "  -6.18542552e-01 -1.47315651e-01 -5.30723214e-01  9.00456548e-01\n",
      "  -5.07012665e-01 -3.95729333e-01 -3.64810526e-02 -4.39003944e-01\n",
      "  -5.66668697e-02 -7.64991522e-01 -1.15275931e+00 -5.64237714e-01\n",
      "   2.25647733e-01 -8.70940804e-01 -1.07499695e+00 -3.87252483e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> \n",
    "<a id = \"1.0\"></a>\n",
    "<p style=\"font-size:25px;color:#16268a;font-family:'Avantgarde';text-align:center;border-radius:5px;\">\n",
    "<strong>______________________________</strong></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MlLab",
   "language": "python",
   "name": "mllab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
