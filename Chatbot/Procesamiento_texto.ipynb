{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gugui/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/gugui/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de preprocesamiento de texto\n",
    "\n",
    "def preprocesar_texto(texto):\n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Eliminar las stopwords\n",
    "    stop_words = set(stopwords.words('spanish')) \n",
    "    palabras = texto.split()\n",
    "    palabras = [palabra for palabra in palabras if palabra not in stop_words]\n",
    "\n",
    "    # Lematización\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    palabras_lematizadas = [lemmatizer.lemmatize(palabra) for palabra in palabras]\n",
    "\n",
    "    # Volver a unir las palabras lematizadas en un solo texto\n",
    "    return ' '.join(palabras_lematizadas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importa JSON\n",
    "import json\n",
    "\n",
    "# Cargar el archivo JSON\n",
    "with open(\"grafo_hierarquia.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    grafo_json = json.load(file)\n",
    "\n",
    "# Crear una lista de nodos con categorías, subcategorías y productos\n",
    "nodos = []\n",
    "for cultivo, data in grafo_json.items():\n",
    "    nodos.append(cultivo)  # Agregar el nombre del cultivo\n",
    "    if 'Productos' in data:\n",
    "        for producto in data['Productos']:\n",
    "            nodos.append(producto)  # Agregar cada producto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gugui/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cargar el modelo de embeddings de frases\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el preprocesamiento y convertir los nodos procesados a embeddings\n",
    "nodos_procesados = [preprocesar_texto(nodo) for nodo in nodos]\n",
    "nodos_embeddings = model.encode(nodos_procesados)\n",
    "nodos_embeddings = np.array(nodos_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index = faiss.IndexFlatL2(nodos_embeddings.shape[1])\n",
    "index.add(nodos_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_rag(consulta, index, nodos, model, top_k=5):\n",
    "    # Generar el embedding de la consulta\n",
    "    consulta_embedding = model.encode([preprocesar_texto(consulta)])\n",
    "    \n",
    "    # Buscar en el índice los nodos más cercanos\n",
    "    D, I = index.search(np.array(consulta_embedding), k=top_k)  \n",
    "    \n",
    "    # Mostrar los resultados\n",
    "    resultados = []\n",
    "    for i in range(top_k):\n",
    "        nodo = nodos[I[0][i]]\n",
    "        distancia = D[0][i]\n",
    "        resultados.append((nodo, distancia))\n",
    "        print(f\"Resultado {i + 1}: {nodo} (Distancia: {distancia:.4f})\")\n",
    "    \n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b781b1c6a8416db885fa36006033c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86ff689dc8241c0bd27b067b3e1af38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cd38ce2e354c7dba8aa0356aad7d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67568404ea2748de9128e6e1cda53573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7040a2bfcb45ef821b7511e291e7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409b2e5bb37048279bdb2e0c4c3ea543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60284345d4604270bcb71730d9ac0115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, ¿cómo estás?\n",
      "\n",
      "I'm sorry, I'm sorry.\n",
      "\n",
      "I'm sorry, I'm sorry.\n",
      "\n",
      "I'm sorry, I'm sorry.\n",
      "\n",
      "I'm sorry, I'm sorry\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Cargar el modelo y el tokenizador de GPT-2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "input_text = \"Hola, ¿cómo estás?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generar una predicción\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decodificar y mostrar el resultado\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión después de reducción manual: (384,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Supongamos que consulta_embedding tiene una dimensión de 768\n",
    "consulta_embedding = torch.tensor(consulta_embedding)\n",
    "\n",
    "# Reducir la dimensión a 384\n",
    "consulta_embedding_384 = consulta_embedding[:384].numpy()  # Tomar solo los primeros 384 elementos\n",
    "\n",
    "print(f\"Dimensión después de reducción manual: {consulta_embedding_384.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta: Spodoptera\n",
      "Resultado 1: Imunit® (Distancia: 2517.7275)\n",
      "Resultado 2: Vid (Distancia: 2521.0815)\n",
      "Resultado 3: Kit Sistiva®  (Distancia: 2521.4136)\n",
      "Resultado 4: Kit Sistiva®  (Distancia: 2521.4136)\n",
      "Resultado 5: Bellis® (Distancia: 2521.8257)\n",
      "Respuesta generada: Imunit® Vid Kit Sistiva®  Kit Sistiva®  Bellis® Viva™  Vivi® Sostiva  Sotiva ® Vivo®\n",
      "\n",
      "Vivis™ VIVi™ Sustiva Vividiva\n",
      ".\n",
      ",\n",
      ":\n",
      "-\n",
      "The Vixen® is a brand new product that is designed to be used with the VIXEN®. The VXVIVI® has been designed for use with VVIXE® and VXXV® products. VxVixE is an innovative and innovative product designed specifically for the use of VxxVX®, VxxxV, and XxxX. It is intended to provide a high quality, high performance, low cost, non-toxic, environmentally friendly, safe, portable, easy to use, affordable, convenient, reliable, durable, eco-friendly, ergonomic, light weight, lightweight, compact, versatile, efficient,\n"
     ]
    }
   ],
   "source": [
    "# Definir el número de nodos más cercanos que deseas obtener\n",
    "top_k = 5\n",
    "\n",
    "# Buscar en el índice FAISS los 5 nodos más cercanos\n",
    "D, I = index.search(np.array([consulta_embedding_384]), k=top_k)  # Buscar los 5 nodos más cercanos\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Consulta: {consulta}\")\n",
    "for i in range(top_k):\n",
    "    print(f\"Resultado {i + 1}: {nodos[I[0][i]]} (Distancia: {D[0][i]:.4f})\")\n",
    "\n",
    "# Concatenar los nodos más cercanos para formar un contexto\n",
    "contexto = \" \".join([nodos[I[0][i]] for i in range(top_k)])\n",
    "\n",
    "# Tokenizar el contexto\n",
    "input_ids = tokenizer.encode(contexto, return_tensors=\"pt\")\n",
    "\n",
    "# Generar una respuesta\n",
    "outputs = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decodificar y mostrar la respuesta generada\n",
    "respuesta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Respuesta generada: {respuesta}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
